{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(keras)\n",
    "library(reticulate)\n",
    "library(ggplot2)\n",
    "library(gridExtra)\n",
    "use_condaenv(\"r-tensorflow\")\n",
    "use_session_with_seed(7)\n",
    "options(keras.view_metrics = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2.1 - Introduction to convnets\n",
    "\n",
    "This notebook contains the code samples found in Chapter 5, Section 1 of [Deep Learning with R](https://www.manning.com/books/deep-learning-with-r). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "----\n",
    "\n",
    "We're about to dive into the theory of what convnets are and why they have been so successful at computer vision tasks. But first, let's take a practical look at a simple convnet example. It uses a convnet to classify MNIST digits, a task we performed in chapter 2 using a densely connected network (our test accuracy then was 97.8%). Even though the convnet will be basic, its accuracy will blow out of the water that of the densely connected model from notebook 1.1.\n",
    "\n",
    "The following lines of code show you what a basic convnet looks like.  It's a stack of `layer_conv_2d()` and `layer_max_pooling_2d()` layers. You'll see in a minute exactly what they do.\n",
    "\n",
    "Importantly, a convnet takes as input tensors of shape `(image_height, image_width, image_channels)` (not including the batch dimension). In this case, we'll configure the convnet to process inputs of size `(28, 28, 1)`, which is the format of MNIST images. We do this by passing the argument `input_shape = c(28, 28, 1)` to the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- keras_model_sequential() %>% \n",
    "  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = \"relu\",\n",
    "                input_shape = c(28, 28, 1)) %>% \n",
    "  layer_max_pooling_2d(pool_size = c(2, 2)) %>% \n",
    "  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = \"relu\") %>% \n",
    "  layer_max_pooling_2d(pool_size = c(2, 2)) %>% \n",
    "  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = \"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Let's display the architecture of our convnet so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can see above that the output of every `Conv2D` and `MaxPooling2D` layer is a 3D tensor of shape `(height, width, channels)`. The width \n",
    "and height dimensions tend to shrink as we go deeper in the network. The number of channels is controlled by the first argument passed to \n",
    "the `Conv2D` layers (e.g. 32 or 64).\n",
    "\n",
    "The next step would be to feed our last output tensor (of shape `(3, 3, 64)`) into a densely-connected classifier network like those you are \n",
    "already familiar with: a stack of `Dense` layers. These classifiers process vectors, which are 1D, whereas our current output is a 3D tensor. \n",
    "So first, we will have to flatten our 3D outputs to 1D, and then add a few `Dense` layers on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- model %>% \n",
    "  layer_flatten() %>% \n",
    "  layer_dense(units = 64, activation = \"relu\") %>% \n",
    "  layer_dense(units = 10, activation = \"softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to do 10-way classification, so we use a final layer with 10 outputs and a softmax activation. \n",
    "\n",
    "Now here's what our network looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our `(3, 3, 64)` outputs were flattened into vectors of shape `(576)`, before going through two `Dense` layers.\n",
    "\n",
    "Now, let's train our convnet on the MNIST digits. We will reuse a lot of the code we have already covered in the previous MNIST example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist <- dataset_mnist()\n",
    "c(c(train_images, train_labels), c(test_images, test_labels)) %<-% mnist\n",
    "\n",
    "train_images <- array_reshape(train_images, c(60000, 28, 28, 1))\n",
    "train_images <- train_images / 255\n",
    "\n",
    "test_images <- array_reshape(test_images, c(10000, 28, 28, 1))\n",
    "test_images <- test_images / 255\n",
    "\n",
    "train_labels <- to_categorical(train_labels)\n",
    "test_labels <- to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model %>% compile(\n",
    "  optimizer = \"rmsprop\",\n",
    "  loss = \"categorical_crossentropy\",\n",
    "  metrics = c(\"accuracy\")\n",
    ")\n",
    "              \n",
    "history = model %>% fit(\n",
    "  train_images, train_labels, \n",
    "  epochs = 1, batch_size=64,\n",
    "  validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2> As you can see each epoch takes around 1 minute to run and we do not have time to train enough epochs, so we load a model that was trained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- load_model_hdf5(\"../data/models/2-1-GPU.h5\")\n",
    "history  <- py_load_object('../data/models/2-1-GPU-history.pk')\n",
    "df <- data.frame(val_loss=unlist(history$val_loss), val_acc=unlist(history$val_acc), loss=unlist(history$loss), acc=unlist(history$acc), epochs=seq(length(history$val_loss)))\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(paste('val_loss:',df$val_loss[df$epoch==100],'\\n'))\n",
    "cat(paste(' val_acc:',df$val_acc[df$epoch==100],'\\n'))\n",
    "cat(paste('    loss:',df$loss[df$epoch==100],'\\n'))\n",
    "cat(paste('     acc:',df$acc[df$epoch==100],'\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=6)\n",
    "\n",
    "p1 <- ggplot(df, aes(x=epochs)) +\n",
    "  geom_point(aes( y=loss, colour = \"Trainig loss\")) +\n",
    "  geom_line(aes(y=val_loss,colour = \"Validation loss\")) +\n",
    "  scale_colour_manual(\"\",values=c(\"blue\",\"orange\"))\n",
    "p2 <- ggplot(df, aes(x=epochs)) +\n",
    "  geom_point(aes( y=acc, colour = \"Training acc\")) +\n",
    "  geom_line(aes(y=val_acc,colour = \"Validation acc\")) +\n",
    "  scale_colour_manual(\"\",values=c(\"blue\",\"orange\"))\n",
    "\n",
    "grid.arrange(p1,p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Let's evaluate the model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results <- model %>% evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our densely-connected network had a test accuracy of 97.8%, our basic convnet has a test accuracy of 99.2%: we decreased our error rate by 68% (relative). Not bad! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = model %>% predict_classes(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_class = 3   # <============== Change me to see different predictions.\n",
    "\n",
    "# extract the prediction, we created a 10 class matrix earlier, so we need to figure out\n",
    "# which class was predicted for the chosen image (ie which entry is 1 as opposed to 0)\n",
    "predict = which.max((test_labels)[valid_class,])-1\n",
    "m <-  t(apply(as.matrix(test_images[valid_class, , ,]), 2, rev))\n",
    "\n",
    "# Plotting the test image\n",
    "options(repr.plot.width=2, repr.plot.height=2)\n",
    "par(oma=c(0,0,0,0), mar=c(0,0,2,0))\n",
    "image(m, asp=1, axes=FALSE)\n",
    "title(main =  paste(\"Predicted:\",classes[valid_class],\"| Actual:\",predict), cex.main=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
